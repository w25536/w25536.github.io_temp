---
layout: page
title: LLM day 1
description: ""
headline: ""
tags:
  - python
  - 파이썬
  - torchtext
  - pytorch
  - 파이토치
  - 전처리
  - data
  - science
  - 데이터
  - 분석
  - 딥러닝
  - 딥러닝
  - 자격증
  - 머신러닝
  - 빅데이터
categories: llm
comments: true
published: true
---

김명욱 강사님

5000차원

Vectorization -> Vector Dataset -> XXX차원 데이터

## Tokenizer

<https://platform.openai.com/tokenizer>

![](https://cdn.mathpix.com/snip/images/pOZLDQV0dlOtvlgHPNlgda6G4TUcG8cNias2e8IaUBo.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/Nie7JxxMj4Pdvk6N8VO6TP0szn7OdXLwgFmn5SYSLKI.original.fullsize.png)

tokenized 단어를 보면 한단어이지만 ized 붙으면서 의미가 달라지기 떄문에 따로 처리함

1.03 하면 대충 단어수가 맞음~

영어는 그러저럭 처리를 하지만 한국어는?

![](https://cdn.mathpix.com/snip/images/OEysQOomNPwKD2Z_Hi-reNcMUV-lHt6Zd448rZxWGYc.original.fullsize.png)

엉망으로 나온다 유니코드를 동장 하는거는 tokenizer는  GPU 파워로 돌리는거다.

![](https://cdn.mathpix.com/snip/images/8n747fmz6gzKQsM3NYSFspqhRpGN7Se8NzFuHYGskBw.original.fullsize.png)

77% 토큰나이저 개선되어 위에 보면 그나마 괜찮다.  전 30 tokens 사용되고 보면 현재는 18토큰이 사용된다.

다음은 가격을 알아보자 ~

<https://openai.com/api/pricing/>

![](https://cdn.mathpix.com/snip/images/AzEruQC7bhmFw8dcIOFhI-5vajGNBuJUsoWEB-ZN03I.original.fullsize.png)

Transformer 모델을 사용하면서 느낀 점이 있다면, 바로 디코더 과정의 복잡성과 그로 인한 비용 문제입니다. 인코더와 디코더를 비교해보면, 디코딩 파트가 훨씬 크고 복잡해 답변을 처리할 때 비용이 급증하게 됩니다.

이와 달리 Batch API를 활용하면 한 번에 여러 요청을 처리할 수 있어 비용 효율성이 크게 향상됩니다. 예를 들어, 즉각적인 응답이 필요 없는 작업이나 검열같이 실시간성이 덜 요구되는 경우, 배치 처리 방식이 적합합니다. 반면에 상담처럼 즉각적인 응답이 필수적인 서비스에서는 실시간 처리가 여전히 요구됩니다.

또한, 다양한 유명 언어 모델을 모아 평가하고 기록하는 사이트도 있으니, 인공지능의 최신 동향과 모델 성능을 꾸준히 체크해보시길 권장합니다.

<https://artificialanalysis.ai/>

DeepSeek R1  - 0 강화 학습으로 학습이 시키면 0 이 붙는다

SLM 모델은  mini

DeepSeek R1 모델이 o1 모델을 사용 했다는 근거 발견되어 소송을 준비 중이라고 한다...

구글은 학습을 시키는 메카니즘이 어려워서 천천이 QUALITY를 높이고 있는 중이다.

Gemini 가 억지로 따라 오다가 무리 할

전세계 말이 안돼는 단어만 모아서 Gemini라는 단어가 생성 했다고 한다.

Claude 뒤를 봐주는 회사 없어서 약하다

bing은 ... 생략 ㅋ

SPEED  높을 수로 좋다고 한다.

어쨰든 돌아와 토큰으로 LLM은 움직인다

4070 쓸수 없는이유 vram이 낮아서 돌릴 수 없다.  하지만 DeepSeek은 가능하다 설명하다

애저에서 한번 만들어보자

![](https://cdn.mathpix.com/snip/images/UxoekKZ7f8xQgl8-rKY-b0cdJw_PwEEH8q5LO68OR4w.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/UxoekKZ7f8xQgl8-rKY-b0cdJw_PwEEH8q5LO68OR4w.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/bTOhh6TO-4Ghq4Zd05CxHmH8bBdMU6nKJnKB6ikbI5A.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/c1gSC9FPZZ7CZF4yOkPZtYhHRRAjtFn9XhcVVNgnZXw.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/rBsvHqkh-gXfvrF014VtPAnJSyxfoPDyFd70dAONDlA.original.fullsize.png)

미세 조정된 모델 배포 = fine-tunning

1. 외만하면 다른 방법을 사용하는게 좋다
2. fine tunnning할때는 사투리
3. 데이터는 json 형태로 만들어야 되고 그 시간은 프로젝트 기간의 80% 시간을 잡아 먹으니 하지마라
4. fine-tunning 음식 데이터 200만원 나옴 하지 않는게 좋다

![](https://cdn.mathpix.com/snip/images/GkXls1BY52WZt3hFE4vH3kWDV1HGWy-uAiZV_RvaFLA.original.fullsize.png)

preview
01  이런건 비용 많이 나오니 사용하지 말아라

![](https://cdn.mathpix.com/snip/images/TEG_mPPYxvmwb-9jcMLVvFz8LxTxbLnIKwp2HHRUUYc.original.fullsize.png)

chat completion

![[CleanShot 2025-02-03 at 12.00.51@2x.png]]

![](https://cdn.mathpix.com/snip/images/0D2M2YXWwO8nsZapzREbDSAPhFgWZaOidQ8sOJeZbBU.original.fullsize.png)

전체 일괄 처리, 즉 배치(batch) 처리란 모든 작업을 한 번에 처리해서 비용은 저렴하지만 결과가 언제 도착할지 없다는 그 불확실성이 ...

![](https://cdn.mathpix.com/snip/images/B_awp9lugHcls7Mgy2IKR61d5Phf4IemLUGMv2NJJro.original.fullsize.png)

# Azure OpenAI 모델 버전과 속도 제한 이해하기

Azure OpenAI를 사용하면서 가장 중요한 것은 모델 버전 선택과 속도 제한 설정입니다. 이번 포스트에서는 이 두 가지 핵심 요소에 대해 자세히 알아보겠습니다.

## RPM(Request Per Minute)이란?

RPM은 분당 처리할 수 있는 토큰의 수를 의미합니다. 이는 API 호출의 속도를 제한하는 중요한 지표인데요. RPM이 높을수록 더 많은 요청을 처리할 수 있지만, 설정값을 초과하면 500 서버 에러가 발생할 수 있으니 주의가 필요합니다.

## 콘텐츠 필터링 시스템

Azure OpenAI는 강력한 콘텐츠 필터링 시스템을 갖추고 있습니다. 특히 RLHF(Reinforcement Learning from Human Feedback) 필터가 적용된 InstructGPT를 통해 부적절한 콘텐츠를 효과적으로 걸러냅니다.

### DefaultV2 필터

기본 제공되는 DefaultV2 필터는 모델 위에 추가적인 필터링 레이어를 적용할 수 있는 옵션을 제공한다. 더 나아가 사용자의 필요에 따라 커스텀 필터를 구성할 수도 있어, 더욱 세밀한 콘텐츠 관리가 가능하다.

![](https://cdn.mathpix.com/snip/images/ULCPfEtovEbTkdnVqG7HFzMl2eCVq3coY9hWpg5Vlpg.original.fullsize.png)

![](https://cdn.mathpix.com/snip/images/LutWy5z60CQKn_Pb838B0YrgVh78bW6rC1e5eCd6RyE.original.fullsize.png)




백터화가 잘 안돼는 즉 토큰과가 잘 안될  경우 Hallcination 같은 상황이 생긴다. 

언어모델은 비지도 학습이다 auto regression  이용한다


![](https://cdn.mathpix.com/snip/images/8jdF6Sn_A4gdG11u3nAuVNSsKi6ZfJjdBJOFFLDfwe8.original.fullsize.png)



python 3.11 추천하다 하지만 지금 내가 하는 공간은 



![](https://cdn.mathpix.com/snip/images/XBdqrVAQQVvaviV6J-hfDaD3D8B0st-UQTtsweO-WBA.original.fullsize.png)

https://labuser22-aiservice-009.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview


## Streamlit



![](https://cdn.mathpix.com/snip/images/OiZZ3wXNZ8SJawbGgiQ6hzlcBYPdq9QTYDoM_dxRYxw.original.fullsize.png)


![](https://cdn.mathpix.com/snip/images/uszvvv4_CtcgAM6cqQq1_1U6V-WmJXLUcFZR0Kv4mGM.original.fullsize.png)

```python
img_width = st.slider("이미지 너비 조절", min_value=100, max_value=1000, value=600)
st.image("https://cdn.hankooki.com/news/photo/202409/192597_266586_1726985307.jpg", caption="Fetched Image", width=img_width)


query = st.text_input("궁금한걸 물어보세요!: ")

button_click = st.button("질문")

if button_click:
    response = openai.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "your are helpful assistant"},
            {"role": "user", "content": query},
        ],
    )

    st.write(response.choices[0].message.content)

```

![](https://cdn.mathpix.com/snip/images/ejZTJ5oPSgR1ixLxBuEpS7rWXTY279tdRSCWrPDpuUQ.original.fullsize.png)


CUDA 병렬로 실행이 가능하고 vector 

8장의 GPU 한번에 돌릴 수 있다. 

NVLINK GPU를 연결 시키는 프로그램이다 


반도체는 대채로 APU 위주로 발전해 왔다. 



```python
import openai
import streamlit as st
from dotenv import load_dotenv
import os

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_type = os.getenv("OPENAI_API_TYPE")
openai.api_version = os.getenv("OPENAI_API_VERSION")
openai.azure_endpoint = os.getenv("OPENAI_ENDPOINT")

MODEL_NAME = os.getenv("MODEL_NAME")

subject = st.text_input("시의 주제를 입력해주세요!: ")
content = st.text_area("시의 내용을 입력해주세요!: ")


button_click = st.button("질문")

if button_click:
    with st.spinner("잠시만 기다려주세요..."):
        response = openai.chat.completions.create(
            model=MODEL_NAME,
            temperature=1,
            messages=[
                {"role": "system", "content": "your are helpful assistant"},
                {"role": "user", "content": "시의 제목은"+subject},
                {"role": "user", "content": "시의 내용은"+content},
                {"role": "user", "content": "시를 지어줘"},
            ],
        )

    st.write(response.choices[0].message.content)


    st.success("Done")```


temperate가 1이 넘어가면 고열로 생각하고 이상한 헛소리를한다. 



